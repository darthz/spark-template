# ğŸ§  InstruÃ§Ãµes para Copiloto de IA - Projeto de Engenharia de Dados

## ğŸ“ Contexto do UsuÃ¡rio

- Ãrea: Engenharia de Dados
- Linguagem principal: Python
- Framework de processamento: PySpark (versÃ£o 3.5.5)
- Armazenamento de dados: S3 OVH (`https://s3.bhs.io.cloud.ovh.net/`)
- Banco de destino: PostgreSQL
- ExecuÃ§Ã£o em terminal: WSL (shell POSIX)

---

## ğŸ“ OrganizaÃ§Ã£o de CÃ³digo

- Sempre que for iniciar uma sessÃ£o Spark, use:
  
  ```python
  from src.utils.spark_handler import get_spark_session

  spark = get_spark_session()
  ```

  **Nunca** utilizar diretamente `SparkSession.builder...` fora do `spark_handler.py`.

- Todas as variÃ¡veis devem ser declaradas no inÃ­cio do script para facilitar manutenÃ§Ã£o e reatribuiÃ§Ã£o de valores.

---

## ğŸ§° Bibliotecas padrÃ£o

- `loguru` para logging
- `tqdm` para barras de progresso
- InÃ­cio de scripts que usam ambas:

  ```python
  from loguru import logger
  from tqdm import tqdm

  logger.remove()
  logger.add(lambda msg: tqdm.write(msg, end=""), colorize=True)
  ```

---

## ğŸ”„ Comportamentos desejados do copiloto

- Sempre melhorar os prompts fornecidos pelo usuÃ¡rio para deixÃ¡-los mais claros, especÃ­ficos e eficientes.
- Em tarefas de refatoraÃ§Ã£o ou geraÃ§Ã£o de cÃ³digo:
  - Explicar brevemente o motivo das sugestÃµes
  - Sugerir nomes claros e semÃ¢nticos para variÃ¡veis e funÃ§Ãµes
- Sugerir boas prÃ¡ticas sempre que possÃ­vel (ex: uso de tipagem, docstrings e logs)

---

## ğŸ“ˆ PreferÃªncias para Scripts e Pipelines

- Utilizar `tqdm` para indicar progresso de laÃ§os ou etapas do pipeline
- Ao lidar com arquivos S3:
  - Preferir uso de `boto3` (ou `awscli` em shell)
- Scripts devem ser modulares, com funÃ§Ãµes bem separadas
- Utilizar `try-except` onde houver riscos de falhas externas (ex: leitura de arquivos, conexÃµes etc.)

---

## ğŸŒ Exemplos de S3

- Caminhos sempre estruturados por camada (ex: `raw/`, `trusted/`, `refined/`, `service/`)
- PreferÃªncia por nomes de arquivos e pastas que incluam data (`YYYYMMDD`) para controle de versÃ£o

---

## âœ… Estilo e ConvenÃ§Ãµes

- Usar snake_case para variÃ¡veis e funÃ§Ãµes
- Usar aspas simples para strings por padrÃ£o (`'string'`)
- Usar f-strings ao invÃ©s de `.format()` ou concatenaÃ§Ã£o para interpolar variÃ¡veis

---

## ğŸ“„ Estrutura tÃ­pica de script

```python
# Imports
from loguru import logger
from tqdm import tqdm
from src.utils.spark_handler import get_spark_session
# outras libs...

# Logger setup
logger.remove()
logger.add(lambda msg: tqdm.write(msg, end=""), colorize=True)

# VariÃ¡veis (paths, flags, parÃ¢metros)
S3_PATH = 's3://drivalake/raw/ambiental/...'
POSTGRES_CONN = 'postgresql://...'

# Spark session
spark = get_spark_session()

# FunÃ§Ãµes
def transform_data(df):
    ...

# ExecuÃ§Ã£o principal
if __name__ == '__main__':
    ...
```