# 🧠 Instruções para Copiloto de IA - Projeto de Engenharia de Dados

## 📍 Contexto do Usuário

- Área: Engenharia de Dados
- Linguagem principal: Python
- Framework de processamento: PySpark (versão 3.5.5)
- Armazenamento de dados: S3 OVH (`https://s3.bhs.io.cloud.ovh.net/`)
- Banco de destino: PostgreSQL
- Execução em terminal: WSL (shell POSIX)

---

## 📁 Organização de Código

- Sempre que for iniciar uma sessão Spark, use:
  
  ```python
  from src.utils.spark_handler import get_spark_session

  spark = get_spark_session()
  ```

  **Nunca** utilizar diretamente `SparkSession.builder...` fora do `spark_handler.py`.

- Todas as variáveis devem ser declaradas no início do script para facilitar manutenção e reatribuição de valores.

---

## 🧰 Bibliotecas padrão

- `loguru` para logging
- `tqdm` para barras de progresso
- Início de scripts que usam ambas:

  ```python
  from loguru import logger
  from tqdm import tqdm

  logger.remove()
  logger.add(lambda msg: tqdm.write(msg, end=""), colorize=True)
  ```

---

## 🔄 Comportamentos desejados do copiloto

- Sempre melhorar os prompts fornecidos pelo usuário para deixá-los mais claros, específicos e eficientes.
- Em tarefas de refatoração ou geração de código:
  - Explicar brevemente o motivo das sugestões
  - Sugerir nomes claros e semânticos para variáveis e funções
- Sugerir boas práticas sempre que possível (ex: uso de tipagem, docstrings e logs)

---

## 📈 Preferências para Scripts e Pipelines

- Utilizar `tqdm` para indicar progresso de laços ou etapas do pipeline
- Ao lidar com arquivos S3:
  - Preferir uso de `boto3` (ou `awscli` em shell)
- Scripts devem ser modulares, com funções bem separadas
- Utilizar `try-except` onde houver riscos de falhas externas (ex: leitura de arquivos, conexões etc.)

---

## 🌐 Exemplos de S3

- Caminhos sempre estruturados por camada (ex: `raw/`, `trusted/`, `refined/`, `service/`)
- Preferência por nomes de arquivos e pastas que incluam data (`YYYYMMDD`) para controle de versão

---

## ✅ Estilo e Convenções

- Usar snake_case para variáveis e funções
- Usar aspas simples para strings por padrão (`'string'`)
- Usar f-strings ao invés de `.format()` ou concatenação para interpolar variáveis

---

## 📄 Estrutura típica de script

```python
# Imports
from loguru import logger
from tqdm import tqdm
from src.utils.spark_handler import get_spark_session
# outras libs...

# Logger setup
logger.remove()
logger.add(lambda msg: tqdm.write(msg, end=""), colorize=True)

# Variáveis (paths, flags, parâmetros)
S3_PATH = 's3://drivalake/raw/ambiental/...'
POSTGRES_CONN = 'postgresql://...'

# Spark session
spark = get_spark_session()

# Funções
def transform_data(df):
    ...

# Execução principal
if __name__ == '__main__':
    ...
```